***

# ОТЧЕТ

## Лабораторная работа №8: Явная разностная схема (1D) и обмен граничными значениями (MPI Sendrecv)

### Сведения о студенте

* **Дата:** 2025-12-28
* **Семестр:** 1
* **Группа:** ПИН-м-о-25-1 (1)
* **Дисциплина:** Параллельные вычисления
* **Студент:** Мизин Глеб Егорович

***

## 1. Цель работы

Реализовать параллельное вычисление решения одномерной нестационарной задачи (явная разностная схема) с распараллеливанием по пространственной координате. Организовать обмен граничными значениями между соседними процессами с помощью `Sendrecv`, измерить время выполнения при разном числе процессов и оценить ускорение/эффективность.

---

## 2. Постановка задачи (кратко)

Решается задача на сетке по `x` и `t`. На каждом шаге по времени вычисляется следующий слой `u^{m+1}` по явной формуле, которая использует значения `u^m` в соседних узлах (`i-1`, `i`, `i+1`).
Для корректного вычисления на границе блока, каждому процессу нужны значения соседнего процесса (ghost-ячейки).

---

## 3. Идея параллельного алгоритма

1. Узлы по `x` (всего `N+1`) делятся между процессами (каждый процесс хранит непрерывный блок узлов).
2. Каждый процесс хранит локальный массив с двумя ghost-ячейками: слева и справа.
3. На каждом шаге времени:

   * выполняется обмен граничных значений с соседями через `Sendrecv` для заполнения ghost-ячейки;
   * после этого вычисляется новый слой по явной формуле;
   * задаются граничные условия на концах отрезка (`x=0` и `x=1`) на соответствующих процессах.
4. Время измеряется как максимум по процессам (`Allreduce(MAX)`).
5. Для контроля выводится `checksum` (сумма значений финального слоя).

---

## 4. Параметры эксперимента

Использованы параметры, обеспечивающие устойчивость (иначе решение может уходить в `nan`):

* `N = 800`
* `M = 50000`
* `T = 1.0`
* `eps = 10^(-1.5)` (по умолчанию в программе)

---

## 5. Команды запуска

```bash
mpiexec -n 1 python lr8_heat1d_explicit_sendrecv.py --N 800 --M 50000 --T 1.0 --verify
mpiexec -n 2 python lr8_heat1d_explicit_sendrecv.py --N 800 --M 50000 --T 1.0
mpiexec -n 4 python lr8_heat1d_explicit_sendrecv.py --N 800 --M 50000 --T 1.0
mpiexec -n 8 python lr8_heat1d_explicit_sendrecv.py --N 800 --M 50000 --T 1.0
```

---

## 6. Результаты

### 6.1. Время выполнения

Время: `compute_time_max` (максимум по процессам).

|  p | T(p), сек |
| -: | --------: |
|  1 | 42.762782 |
|  2 | 23.279288 |
|  4 | 14.027531 |
|  8 | 11.246174 |

Контрольная сумма (пример):

* p=1: `5.684342e-13`
* p=2: `5.684342e-13`
* p=4: `5.684342e-13`
* p=8: `5.115908e-13`

Небольшие расхождения возможны из-за различного порядка операций с плавающей точкой.

### 6.2. Ускорение и эффективность

Формулы:

* `Speedup S(p) = T(1) / T(p)`
* `Efficiency E(p) = S(p) / p`

Где `T(1) = 42.762782`.

|  p | T(p), сек | Speedup S(p) | Efficiency E(p) |
| -: | --------: | -----------: | --------------: |
|  1 | 42.762782 |        1.000 |           1.000 |
|  2 | 23.279288 |        1.837 |           0.919 |
|  4 | 14.027531 |        3.049 |           0.762 |
|  8 | 11.246174 |        3.802 |           0.475 |

### 6.3. Проверка корректности

Для `p=1` включалась проверка `--verify` (сравнение с последовательным расчётом):

* `max abs diff vs seq = 0.000e+00`

---

## 7. Заключение

Реализована параллельная явная схема с распараллеливанием по `x` и обменом граничных значений между процессами с помощью `Sendrecv`. Получено ускорение при увеличении числа процессов, при этом эффективность падает из-за накладных расходов на обмены и синхронизацию.

---