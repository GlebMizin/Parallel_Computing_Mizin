***

# ОТЧЕТ

## Лабораторная работа №12: Гибридное программирование MPI + потоки (NumPy/OpenBLAS)

### Сведения о студенте

* **Дата:** 2025-12-28
* **Семестр:** 1
* **Группа:** ПИН-м-о-25-1 (1)
* **Дисциплина:** Параллельные вычисления
* **Студент:** Мизин Глеб Егорович

***

## 1. Цель работы

Изучить подход **гибридного параллелизма**:

* **MPI** — параллелизм между процессами (распределённая память),
* **потоки внутри процесса** — через многопоточный BLAS (OpenBLAS в NumPy).

Сравнить производительность матрично-векторного умножения при одинаковом “бюджете” вычислительных потоков, но разном количестве MPI-процессов.

---

## 2. Постановка задачи

Вычислить `y = A_part @ x` на каждом MPI процессе, где каждому процессу соответствует часть строк матрицы.
Затем измерить время ядра и сравнить запуски при разном числе MPI-процессов и числе потоков OpenBLAS.

---

## 3. Реализация (кратко)

* Матрица `A_part` генерируется на каждом процессе (локально).
* Вектор `x` генерируется на root и рассылается всем через `Bcast`.
* Основное ядро: `A_part @ x` (вычисляется через BLAS в NumPy).
* Время измеряется как максимум по процессам: `compute_time_max = Allreduce(MAX)`.
* Дополнительно выводится информация о числе потоков BLAS через `threadpoolctl`.

---

## 4. Среда и версии

* Python: venv (Windows)
* NumPy использует: **OpenBLAS** (по выводу `threadpoolctl`)
* Потоки задавались переменными среды:

  * `OMP_NUM_THREADS`
  * `OPENBLAS_NUM_THREADS`
  * `MKL_NUM_THREADS`

---

## 5. Параметры эксперимента

* `M = 2_000_000`
* `N = 200`
* `reps = 5`

---

## 6. Команды запуска

Примеры:

**MPI-only (8 процессов, 1 поток BLAS):**

```bash
OMP_NUM_THREADS=1
OPENBLAS_NUM_THREADS=1
MKL_NUM_THREADS=1
mpiexec -n 8 python lr12_hybrid_mpi_numpy.py --M 2000000 --N 200 --reps 5
```

**Hybrid (4 процесса × 2 потока):**

```bash
OMP_NUM_THREADS=2
OPENBLAS_NUM_THREADS=2
MKL_NUM_THREADS=2
mpiexec -n 4 python lr12_hybrid_mpi_numpy.py --M 2000000 --N 200 --reps 5
```

**Hybrid (2 процесса × 4 потока):**

```bash
OMP_NUM_THREADS=4
OPENBLAS_NUM_THREADS=4
MKL_NUM_THREADS=4
mpiexec -n 2 python lr12_hybrid_mpi_numpy.py --M 2000000 --N 200 --reps 5
```

---

## 7. Результаты

### 7.1. Время и потоки BLAS

| Конфигурация | MPI procs | BLAS threads | Всего потоков (procs×threads) | compute_time_max (s) |      checksum |
| ------------ | --------: | -----------: | ----------------------------: | -------------------: | ------------: |
| MPI-only     |         8 |            1 |                             8 |             0.518665 |  3.785474e+04 |
| Hybrid       |         4 |            2 |                             8 |             0.531157 |  1.172292e+04 |
| Hybrid       |         2 |            4 |                             8 |             0.519011 | -9.077513e+03 |

Проверка потоков (по `threadpoolctl`):

* OpenBLAS реально переключал `num_threads = 1 / 2 / 4`.

> Примечание: `checksum` может отличаться между запусками, т.к. матрица генерируется локально на процессах и при разном числе MPI-процессов получается другой набор случайных блоков.

### 7.2. Сравнение относительно базового варианта (8×1)

База: `T_base = 0.518665 s` (8 MPI процессов, 1 поток).

| Конфигурация |    T (s) | Speedup = T_base / T |
| ------------ | -------: | -------------------: |
| 8×1          | 0.518665 |                1.000 |
| 4×2          | 0.531157 |                0.977 |
| 2×4          | 0.519011 |                0.999 |

---

## 8. Вывод

При одинаковом общем количестве вычислительных потоков (8) время выполнения для конфигураций **8×1**, **4×2**, **2×4** оказалось очень близким. Существенного выигрыша от перехода к гибридному варианту в данном тесте не получено; накладные расходы MPI/потоков и особенности BLAS приводят к сопоставимым результатам.

---