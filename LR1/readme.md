***

# ОТЧЕТ

## По лабораторной работе №1: Параллельное умножение матрицы на вектор с использованием MPI в Python

### Сведения о студенте

**Дата:** 2025-12-28
**Семестр:** 1
**Группа:** ПИН-м-о-25-1 (1)
**Дисциплина:** Параллельные вычисления
**Студент:** Мизин Глеб Егорович

***

## 1. Цель работы

Освоить базовые принципы программирования в модели передачи сообщений (Message Passing) с использованием библиотеки **mpi4py** для Python и получить практические навыки распараллеливания вычислительной задачи **умножения матрицы на вектор** на системе с распределённой памятью.

## 2. Теоретическая часть

### 2.1. Основные понятия и алгоритмы

Требуется вычислить произведение матрицы (A\in\mathbb{R}^{M\times N}) на вектор (x\in\mathbb{R}^{N}):

[
b = A,x,\quad b\in\mathbb{R}^{M}
]

Параллельный алгоритм (MPI) использует разбиение матрицы по строкам:

1. **Декомпозиция данных.** Матрица (A) делится на блоки строк (горизонтальные полосы). Каждый процесс получает подматрицу
   (A_{part}) размера (local_M\times N).
2. **Распределение данных.** Процесс 0 (root) читает входные данные и распределяет:

   * всем процессам — вектор (x) (одинаковый для всех);
   * каждому процессу — свой блок строк матрицы (A_{part}).
3. **Локальные вычисления.** Каждый процесс вычисляет свою часть результата:
   [
   b_{part} = A_{part} \cdot x
   ]
4. **Сбор результата.** Части (b_{part}) собираются на root в итоговый вектор (b).

Для случая, когда (M) не делится на число процессов (p), используются массивы:

* `rcounts[i]` — число строк, отправляемых процессу `i`
* `displs[i]` — смещение (номер первой строки) блока процесса `i` в глобальной матрице

Пример: при `M=5`, `p=3`
`rcounts = [2, 2, 1]`, `displs = [0, 2, 4]`.

### 2.2. Используемые функции MPI

В реализации использованы:

* `MPI.COMM_WORLD` — глобальный коммуникатор.
* `comm.Get_rank()` — ранг процесса.
* `comm.Get_size()` — число процессов.
* `comm.bcast(obj, root=0)` — рассылка скалярных параметров (размеров `M`, `N`).
* `comm.Bcast(buf, root=0)` — рассылка массивов (вектор `x`) по всем процессам.
* `comm.Scatterv(...)` — распределение блоков матрицы `A` с разным размером у процессов.
* `comm.Gatherv(...)` — сбор частей вектора результата `b_part` разного размера.
* `MPI.Wtime()` — измерение времени.
* `comm.Barrier()` — синхронизация (вокруг измеряемого участка).

---

## 3. Практическая реализация

### 3.1. Структура программы

Проект содержит следующие скрипты:

* `gen_data.py` — генерация тестовых файлов:

  * `in.dat`, `AData.dat`, `xData.dat`
* `seq_matvec.py` — последовательная версия для проверки корректности (`Results_seq.dat`)
* `mpi_scatterv_gatherv.py` — параллельная версия на `Scatterv/Gatherv` (`Results_parallel.dat`)
* `test_mpi.py` — простой тест работоспособности MPI.

### 3.2. Ключевые особенности реализации

1. **Формат входных файлов**

   * `in.dat`:
     1-я строка — `N` (число столбцов матрицы)
     2-я строка — `M` (число строк матрицы)
   * `AData.dat`: `M*N` чисел (матрица в порядке строк, row-major)
   * `xData.dat`: `N` чисел (вектор `x`)

2. **Корректная работа при `M % size != 0`**
   Расчёт `rcounts`/`displs` выполняется на root и рассылается всем процессам.
   Для `Scatterv` матрицы количество элементов вычисляется как `rcounts[i] * N`, смещения — `displs[i] * N`.

3. **Передача размеров матрицы**
   Для рассылки размеров используется `comm.bcast()`, что избавляет от проблем с broadcast numpy-скаляров.

### 3.3. Инструкция по запуску

Пример команд (Windows + MS-MPI, PowerShell, активирован venv):

```bash
# 1) генерация данных
python gen_data.py

# 2) последовательная версия
python seq_matvec.py

# 3) параллельная версия (пример на 4 процессах)
mpiexec -n 4 python mpi_scatterv_gatherv.py

# 4) проверка корректности (сравнение результатов)
python - << 'PY'
import numpy as np
a=np.loadtxt("Results_seq.dat")
b=np.loadtxt("Results_parallel.dat")
print("max abs diff:", np.max(np.abs(a-b)))
PY
```

---

## 4. Экспериментальная часть

### 4.1. Тестовые данные

Входные данные генерируются скриптом `gen_data.py`.
Для адекватных измерений рекомендуется использовать матрицы не меньше **1000×1000**, иначе накладные расходы коммуникаций и Python-диспетчеризации начинают доминировать.

### 4.2. Методика измерений

* Время измерялось через `MPI.Wtime()` вокруг участка **локального вычисления** `b_part = A_part @ x` с синхронизацией `Barrier()`.
* Измерение `compute time` отражает **только вычисления**, без учёта времени чтения файлов и распределения данных.

### 4.3. Результаты измерений

Фактически полученный вывод при запуске на 4 процессах:

```text
compute time ~ 0.000075 s, wrote Results_parallel.dat
```

#### Таблица 1. Время выполнения (секунды)

> Заполните для своих размеров (например, A=1000×1000, B=2000×2000, C=4000×4000).
> В столбце A ниже внесено одно измерение, которое было получено в ходе работы.

| Количество процессов | Размер задачи A | Размер задачи B | Размер задачи C |
| -------------------- | --------------- | --------------- | --------------- |
| 1                    | [замерить]      | [замерить]      | [замерить]      |
| 2                    | [замерить]      | [замерить]      | [замерить]      |
| 4                    | 0.000075        | [замерить]      | [замерить]      |
| 8                    | [замерить]      | [замерить]      | [замерить]      |

#### Таблица 2. Ускорение (Speedup)

Ускорение вычисляется как:

[
S(p) = \frac{T_{seq}}{T_{par}(p)}
]

| Количество процессов | Размер задачи A | Размер задачи B | Размер задачи C |
| -------------------- | --------------- | --------------- | --------------- |
| 1                    | 1.00            | 1.00            | 1.00            |
| 2                    | [посчитать]     | [посчитать]     | [посчитать]     |
| 4                    | [посчитать]     | [посчитать]     | [посчитать]     |
| 8                    | [посчитать]     | [посчитать]     | [посчитать]     |

---

## 5. Визуализация результатов

Для отчёта рекомендуется построить и сохранить в `images/`:

* график времени выполнения от числа процессов;
* график ускорения `Speedup`;
* график эффективности `E(p)=S(p)/p`.

(В эту работу графики не были включены, т.к. требуется предварительно снять серию замеров на нескольких размерах задач.)

---

## 6. Анализ результатов

### 6.1. Анализ производительности

Идеальное ускорение редко достигается из-за:

* накладных расходов на коммуникации (Broadcast/Scatterv/Gatherv);
* синхронизаций (Barrier);
* дисбаланса нагрузки при `M % p != 0` (часть процессов получает на 1 строку больше);
* затрат на ввод/вывод (чтение/запись файлов) и интерпретацию Python.

На малых размерах матрицы наблюдается очень маленькое время вычислений, поэтому ускорение может быть неустойчивым и «шумным».

### 6.2. Сравнение с теоретическими оценками

Вычислительная сложность:

* вычисления: (O(M\cdot N)) суммарно, или (O(local_M\cdot N)) на процесс;
* коммуникации: рассылка `x` — (O(N)), Scatterv матрицы — (O(M\cdot N)) данных, сбор `b` — (O(M)).

При достаточно больших `M,N` вычисления доминируют и ускорение растёт ближе к линейному (до влияния памяти/сети).

### 6.3. Выявление узких мест

Основные «узкие места»:

* пересылка больших блоков матрицы `A` (Scatterv);
* синхронизации и ожидания при сборе результатов (Gatherv);
* I/O при чтении матрицы из файла (если включать в измерение).

---

## 7. Ответы на контрольные вопросы

### Вопрос 1: Что такое ранг процесса и размер коммуникатора?

**Ранг** — уникальный номер процесса внутри `MPI.COMM_WORLD` (0..size−1).
**Размер** — общее число процессов в коммуникаторе.

### Вопрос 2: Чем отличаются Send/Recv и коллективные операции?

`Send/Recv` — операции «точка-точка» между двумя процессами.
Коллективные (`Bcast`, `Scatterv`, `Gatherv`) выполняются одновременно группой процессов и обычно оптимизированы внутри MPI.

### Вопрос 3: Зачем нужен Broadcast в задаче матрица×вектор?

Размеры `M,N` и вектор `x` нужны всем процессам для вычисления `b_part`, поэтому их удобно разослать через `bcast/Bcast`.

### Вопрос 4: Почему используются Scatterv/Gatherv, а не Scatter/Gather?

`Scatter/Gather` предполагают одинаковый размер блоков у всех процессов.
`Scatterv/Gatherv` позволяют распределять/собирать блоки **разного размера**, что нужно при `M % p != 0`.

### Вопрос 5: Что означают массивы rcounts и displs?

`rcounts[i]` — сколько строк (или элементов) отправляется/собирается для процесса `i`.
`displs[i]` — смещение (начальный индекс) блока процесса `i` в глобальном массиве.

### Вопрос 6: Что делает Barrier и зачем он использовался?

`Barrier` синхронизирует процессы: каждый ждёт, пока все дойдут до барьера.
Используется для корректного измерения времени вычисления (чтобы процессы начинали/заканчивали участок одновременно).

### Вопрос 7: Почему ускорение не идеальное (не равно числу процессов)?

Из-за накладных расходов на обмен данными и синхронизацию, дисбаланса нагрузки и доли последовательной части программы (закон Амдала).

### Вопрос 8: Как проверить корректность параллельного результата?

Сравнить `Results_parallel.dat` с результатом последовательной версии `Results_seq.dat`, например по `max abs diff`.

### Вопрос 9: Какие типы данных MPI использовались для numpy-массивов?

Для `float64` использовался `MPI.DOUBLE`, для целых размеров/счётчиков — `MPI.INT` (в связке с `np.int32`).

### Вопрос 10: Как модифицировать алгоритм для очень больших матриц?

Уменьшать I/O (например, бинарные форматы), использовать блочное чтение/распределение, по возможности перекрывать коммуникации вычислениями, и учитывать топологию сети/NUMA.

---

## 8. Заключение

### 8.1. Выводы

Реализованы:

* последовательная версия умножения матрицы на вектор (верификация);
* параллельная версия MPI на Python с использованием **Scatterv/Gatherv**, корректно работающая при произвольных `M` и числе процессов.

### 8.2. Проблемы и решения

* Возникала ошибка broadcast скалярных numpy-значений (`BufferError: scalar buffer is readonly`).
  Решение: рассылать размеры через `comm.bcast()` (Python-объекты), а массивы — через `Bcast`.

### 8.3. Перспективы улучшения

* Снимать замеры на больших размерах матриц и строить графики ускорения/эффективности.
* Добавить сравнение с реализацией на `Send/Recv` (Этап 2) и оценить разницу в коммуникационных затратах.

---

## 9. Приложения

### 9.1. Исходный код

Файлы проекта:

* `gen_data.py`
* `seq_matvec.py`
* `mpi_scatterv_gatherv.py`
* `test_mpi.py`

### 9.2. Используемые библиотеки и версии

* Python 3.10.7 (venv)
* mpi4py (через pip)
* NumPy
* MS-MPI (mpiexec)

### 9.3. Рекомендуемая литература

1. Документация mpi4py — описание API и примеры коллективных операций.
2. Документация MPI (стандарт MPI) — семантика Bcast/Scatter/Gather/Scatterv/Gatherv.
3. Материалы лекций по курсу «Параллельные вычисления» (MPI, закон Амдала, оценка эффективности).

---

*Отчет подготовлен в рамках курса "Параллельные вычисления"*
