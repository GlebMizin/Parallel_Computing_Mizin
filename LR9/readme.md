***

# ОТЧЕТ

## Лабораторная работа №9: Неявная схема (ROS1) и параллельное решение СЛАУ (MPI)

### Сведения о студенте

* **Дата:** 2025-12-28
* **Семестр:** 1
* **Группа:** ПИН-м-о-25-1 (1)
* **Дисциплина:** Параллельные вычисления
* **Студент:** Мизин Глеб Егорович

***

## 1. Цель работы

Реализовать неявную схему интегрирования по времени (ROS1), где на каждом шаге требуется решать трёхдиагональную СЛАУ. Реализовать параллельное решение этой СЛАУ с помощью MPI и измерить время выполнения при разном числе процессов.

---

## 2. Постановка задачи (кратко)

Решается одномерная задача на сетке по `x` и `t`. Для внутренних узлов (без границ) формируется вектор неизвестных `y` и применяется шаг по времени методом ROS1. На каждом шаге необходимо решать систему вида:

`(I - alpha * tau * J) * w1 = f(y, t)`
`y_next = y + tau * w1`

Где:

* `f(y, t)` — правая часть (включая разностные производные),
* `J` — якобиан `df/dy` (в данной задаче трёхдиагональный),
* `tau` — шаг по времени.

---

## 3. Параллельный алгоритм

1. Вектор неизвестных `y` (размер `N-1`) делится по процессам блоками.
2. На каждом шаге времени процессы обмениваются граничными значениями `y` с соседями через `Sendrecv`, чтобы корректно вычислять производные на границе блока.
3. Для каждого процесса формируются локальные диагонали трёхдиагональной матрицы и локальная правая часть.
4. Глобальная трёхдиагональная СЛАУ решается параллельной прогонкой (аналогично ЛР7):

   * локальные прогоночные решения + редуцированная система на root,
   * обмен граничными неизвестными,
   * восстановление решения внутри блока.
5. Получается `w1`, затем выполняется обновление `y_next = y + tau*w1`.

---

## 4. Использованные функции MPI

* `Create_cart` — линейная топология процессов
* `Sendrecv` — обмен граничными значениями
* `Gather/Gatherv` — сбор коэффициентов / сбор решения на root
* `Scatter` — рассылка граничных неизвестных блокам
* `Barrier`, `Allreduce(MAX)` — измерение времени как максимум по процессам

---

## 5. Параметры эксперимента

Для ускорения вычислений использован уменьшенный прогон (по сравнению с “тяжёлым” вариантом из методички):

* `N = 5000`
* `M = 1000`

---

## 6. Запуск

```bash
mpiexec -n 1 python lr9_ros1_implicit_parallel.py --N 5000 --M 1000 --verify
mpiexec -n 2 python lr9_ros1_implicit_parallel.py --N 5000 --M 1000
mpiexec -n 4 python lr9_ros1_implicit_parallel.py --N 5000 --M 1000
mpiexec -n 8 python lr9_ros1_implicit_parallel.py --N 5000 --M 1000
```

---

## 7. Результаты

### 7.1. Время выполнения

Время: `compute_time_max` (максимум по процессам).

|  p | T(p), сек |
| -: | --------: |
|  1 | 22.422024 |
|  2 | 11.386342 |
|  4 |  6.793585 |
|  8 |  5.361057 |

Контрольная сумма (checksum) финального вектора `y` (может немного отличаться из-за порядка операций с float):

* p=1: `8.012648e-10`
* p=2: `4.279173e-10`
* p=4: `3.064997e-10`
* p=8: `4.047251e-10`

### 7.2. Ускорение и эффективность

Формулы:

* `Speedup S(p) = T(1) / T(p)`
* `Efficiency E(p) = S(p) / p`

`T(1) = 22.422024`.

|  p | T(p), сек | Speedup S(p) | Efficiency E(p) |
| -: | --------: | -----------: | --------------: |
|  1 | 22.422024 |        1.000 |           1.000 |
|  2 | 11.386342 |        1.969 |           0.984 |
|  4 |  6.793585 |        3.300 |           0.825 |
|  8 |  5.361057 |        4.183 |           0.523 |

### 7.3. Проверка корректности

Для `p=1` включалась проверка `--verify`:

* `max abs diff vs seq = 0.000e+00`

То есть параллельный расчёт совпал с последовательным.

---

## 8. Заключение

Реализована неявная схема ROS1, в которой на каждом шаге времени решается трёхдиагональная СЛАУ. Решение СЛАУ распараллелено средствами MPI с обменом граничных данных и параллельной прогонкой. Получено ускорение при росте числа процессов, при этом эффективность снижается из-за накладных расходов на обмен и синхронизацию.

---
