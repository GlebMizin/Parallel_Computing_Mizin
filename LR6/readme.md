***
# ОТЧЕТ

## Лабораторная работа №6: Виртуальные топологии MPI (2D-тор) и обмены Sendrecv_replace

### Сведения о студенте

* **Дата:** 2025-12-28
* **Семестр:** 1
* **Группа:** ПИН-м-о-25-1 (1)
* **Дисциплина:** Параллельные вычисления
* **Студент:** Мизин Глеб Егорович

***

## 1. Цель работы

Освоить работу с виртуальными топологиями MPI (декартовая топология), научиться получать соседей процессов и реализовать коллективную операцию суммирования (аналог `Allreduce(SUM)`) с использованием обменов с соседями (`Sendrecv_replace`) на 2D-торе. Сравнить время работы с стандартным `Allreduce`.

---

## 2. Постановка задачи

Для каждого процесса задан локальный вектор `a` длины `L`. Требуется вычислить глобальную сумму по всем процессам:

`sum = a0 + a1 + ... + a(p-1)`  (поэлементно)

Результат должен получиться у всех процессов (аналог `Allreduce(SUM)`).

---

## 3. Реализация

### 3.1. Виртуальная топология 2D-тор

Процессы организуются в решётку `q×q`, где `q^2 = P` (P — число процессов).
Топология создаётся через `Create_cart(..., periods=(True, True))`, что задаёт тор (замкнутые границы).

Соседи процесса получаются через `Shift(direction, disp)`:

* `direction=0` — вверх/вниз
* `direction=1` — влево/вправо

---

### 3.2. Аналог Allreduce(SUM) на торе

Операция реализована в два этапа:

1. Суммирование по строкам тора (кольцевой обмен внутри строки).
2. Суммирование по столбцам тора (кольцевой обмен внутри столбца).

На каждом шаге используется `Sendrecv_replace`, который одновременно отправляет и принимает данные у соседей, обновляя буфер.

Итоговый вектор суммы получается на всех процессах.

---

## 4. Использованные функции MPI

* `Create_cart` — создание декартовой топологии
* `Get_coords` — координаты процесса в решётке
* `Shift` — получение рангов соседей в торе
* `Sub` — подкоммуникаторы строки и столбца
* `Sendrecv_replace` — обмен с соседями “туда-обратно”
* `Barrier` — синхронизация
* `Allreduce(MAX)` — время берётся как максимум по процессам
* `Allreduce(SUM)` — стандартное сравнение (эталон)

---

## 5. Параметры эксперимента

* Число процессов: `P = 9`
* Размер тора: `3×3`
* Длина вектора: `vec = 1024`
* Повторов для замера времени: `reps = 2000`

---

## 6. Запуск

```bash
mpiexec -n 9 python lr6_torus_allreduce.py --vec 1024 --reps 2000
```

---

## 7. Результаты

Измеренные времена (время — максимум по процессам):

* `time_torus_max    = 0.103644 s`
* `time_allreduce_max = 0.274622 s`

Проверка корректности:

* `max abs diff = 1.776e-15`

Вывод: реализация суммирования на торе совпадает со стандартным `Allreduce(SUM)` с высокой точностью.

---

## 8. Анализ

На данном эксперименте реализация на торе оказалась быстрее стандартного `Allreduce`. Возможная причина: алгоритм обменов использует фиксированные обмены только с соседями и хорошо ложится на топологию, а стандартный `Allreduce` может иметь более “тяжёлую” схему обменов для данного MPI/системы.

---

## 9. Заключение

В работе реализована 2D-декартова топология (тор), получены соседи процессов и написан аналог `Allreduce(SUM)` на основе `Sendrecv_replace`. Выполнено сравнение времени со стандартным `Allreduce`, корректность подтверждена малой разницей результатов.

---